Sampling and point estimates
-population is the complete dataset
-sample is only a portion that (ideally) represents the whole population
-use .sample() method on DataFrames and Series to get a random sample of the dataset. Sample rows will be unique instances from the dataframe/series
	-use n= to set number of rows you want in the sample
population parameters are calculations made on the population dataset.
point estimate/sample statistic is a calculation based on the sample dataset
-ideally, sample stats are closed to population stats

convenience sampling
-sample bias occurs when the sample is not a representation of the population as a whole
-sample bias messes with sample stats, making them no longer an accurate representation of the population as a whole
-convenience sampling is collecting data by the easiest method
	-can easily lead to sample bias
-histograms can be used to visualize any selection bias
	-make sure same bins are defined for population histogram and sample histogram

psuedo-random number generation:
true random is expensive and slow, so we use pseudo-random, in which the next 'random' number is determined by the previous one
	-the first random number is determined by the seed
	-setting the seed allows us to create replicability in cases where we do need an element of randomness.
-there are also many functions for generating random functions
	-accessed with numpy.random.func()

Simple random and systematic sampling
simple random:
-start with population, randomly pick one out at a time
	-each row/observation has same chance of being picked
	-sometimes we end up with observations that were close together in the original dataset, and sometimes large chunk of the original dataset didn't have any observations drawn from them
systematic sampling:
-sample the population at regular intervals(ex every 5th record)
	1. define interval
		-pop_size//sample_size
	2.use .iloc[::interval] on dataframe
-problems are that bias can be introduced if dataset is not organized completely randomly
	-should check by creating a scatter plot with the desired measure vs index, and if there is no pattern you're good to go with systematic sampling
-can also randomize row order before sampling.
	df.sample(frac=1) returns a randomly shuffled dataframe containing all the observations in df
	-next reset indicies so that they start from zero again

Stratified and weighted random sampling
-allows sampling of a population with subgroups
	-ex coffee beans grouped by country of origin
-simply using the simple random sampling could result in one or more subgroups being disproportionally represented in the sample
-use .groupby first
	sample = df.groupby(subgroup).sample(frac=, random_state=)
-can also sample equal counts from each group instead of equal proportion
	sample = df.groupby(subgroup).sample(n=, random_state=)
-weighted random sampling
	-we specify weights to adjust the relative probability of a row being sampled
	-first, create a condition (rows[subgroup] == 'group1')
	-create new weight column, use np.where(condition, x, y) where x is the weight for rows that meet the condition, and y is the weight for all other rows
	-pass weight col to weights argment of sample:
		sample = df.groupby(subgroup).sample(frac=, weights='weight')

Cluster sampling:
-cheaper than stratified sampling
stratified sampling:
-split population into subgroups
-use simple random sampling on every subgroup
cluster sampling:
-use simple random sampling to pick some subgroups
-use simple random sampling on only those subgroups

1.Randomly select subgroups
	-subgroup_samp = random.sample(subgroup_list, k=)
2.perform simple random sampling of each group
	df_subgroup_samp = df['subgroup'].isin(subgroup_samp)
	subgroup_cluster = df[df_subgroup_samp]
	use df['subgroup'] = df['subgroup'].cat.remove_unused_categories() to remove levels with 0 rows
cluster sampling is a type of multistage sampling
	-can have more than 2 stages, such as in a countrywide sample

comparing sampling methods:
Assuming there is no sample bias:
means of population, simple random sample, and stratified sample tend to be very close, while cluster sampling means is a little off
	-ultimately, cluster sampling is intend to give us an answer that it 'almost as good' while still being cheaper and using less data

Relative error of point estimates:
-size of sample affects accuracy of point estimates we calculate
-bigger samples are more accurate
relative error is the common metric for assessing the difference between population and sample stat
relative error = 100% * |population_mean-sample_mean|/population_mean

Creating a sampling distribution:
sampling distribution is taking multiple samples of the same size
	-when performing stat analysis, the stats tend to distribute along a bell curve
'A distribution of replicates of sample means, or other point estimates, is known as a sampling distribution. '

Approximate sampling distributions:
-can use expan_grid (pandas function) which can generate combinations for probabilities (dice rolls)
-basically, as number of outcomes increase, it is easier to sample a distribution and sample based on that

np.random.choice(list(range(1,7)),size=4,replace=True).mean() simulates mean of 4 dice rolls

Standard errors and the Central Limit Theorem
gaussian distribution(aka normal distribution) is highly relevant to statistics
-stats of samples tend to follow a gaussian distribution, especially as the number of samples increases
	-this is part of the central limit theorem
Central Limit Theorem: Averages of independent samples have approximately normal distributiosn. As sample size increases, distribution of the averages gets closer to being normally distributed. The wodth of the sampling distribution gets narrower.

Introduction to bootstrapping:
resampling is sampling with replacement
	we can use this to approximate observations not in our dataset
bootstrapping is building a theoretical population from the sample
	-important when we aren't able to sample the population multiple times to create a sampling distribution
Bootstrapping has 3 steps
	1. randomly sample with replacement to get a sample the same size as the original dataset
	2.calculate statistic of interest fort he bootstrap sample
	3. repeat steps 1 and 2 many times to get the sampling distribution
comparing sampling and bootstrap distributions:
bootstrap distribution mean is usually close to the sample mean, but possibly a bit further away from population mean.
	being close to the sample mean isn't necessarily good, especially when the sample was not a good estimate of the population
**bootstrapping can't correct for bias in the sample distributions
standard error is the standard deviation of the statistic of interest
	-standard_error = np.std(bootstrap_distn, ddof=1)
then:
	standard error * np.sqrt(sample size) ~= population standard deviation

confidence intervals:
-values within one standard deviation of the mean, by default
	-depending on your data you may want to alter your confidence interval to reflect different upper and lower percentile bounds

other way to get confidence interval: use inverse cumulative distribution function
-start with pdf/bell curve
-integreat to get area under the bell curve, this is the cdf (cumulative ditribution function)
-flip x and y axes, and now we have inverse cdf
python:
from scipy.stats import norm
norm.ppf(quantile, loc=0, scale=1)
this second way is called the standard error method:
1. calculate point estimate
	point_estimate = np.mean(df_boot_distn)
2. calculate standard error:
	std_error = np.std(df_boot_distn, ddof=1)
3.use norm.ppf
	lower = norm.ppf(lower_quantile, loc = point_estimate, scale = std_error)
	upper = norm.ppf(upper_quantile, loc = point_estimate, scale = std_error)
